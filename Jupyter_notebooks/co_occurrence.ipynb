{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "## Pipeline for identifying reads with multiple SNPs matching the alternate allele\n",
    "## Imports\n",
    "import pandas as pd\n",
    "import pysam\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=False)\n",
    "import ast\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the lookup dictionary from the lookup dataframe, return a dictionary with the chromosome as the key and a set of positions as the value\n",
    "def get_positions_by_chrom_set(lookup_df):\n",
    "    positions_by_chrom = {}\n",
    "    for _, row in lookup_df.iterrows():\n",
    "        chrom, pos = row['Chrom'], row['Pos']\n",
    "        if chrom not in positions_by_chrom:\n",
    "            positions_by_chrom[chrom] = set()\n",
    "        positions_by_chrom[chrom].add(pos)\n",
    "    return positions_by_chrom\n",
    "\n",
    "## Create the lookup dictionary from the lookup dataframe, return a dictionary with the chromosome as the key and a dictionary of positions with ref and alt alleles as the value\n",
    "def get_positions_by_chrom_dict(lookup_df):\n",
    "    positions_by_chrom = {}\n",
    "    for _, row in lookup_df.iterrows():\n",
    "        chrom, pos, ref, alt = row['Chrom'], row['Pos'], row['Ref'], row['Alt']\n",
    "        if chrom not in positions_by_chrom:\n",
    "            positions_by_chrom[chrom] = {}\n",
    "        positions_by_chrom[chrom][pos] = {'ref': ref, 'alt': alt}\n",
    "    return positions_by_chrom\n",
    "    \n",
    "\n",
    "## Filter the reads to only include reads that cover more than 1 SNP position\n",
    "def filter_reads_2SNPs(bam_file, positions_by_chrom, filtered_bam_file):\n",
    "    # Convert positions to a dictionary grouped by chromosome for faster lookup\n",
    "    \n",
    "\n",
    "    # Open the BAM file for reading\n",
    "    bam = pysam.AlignmentFile(bam_file, \"rb\")\n",
    "\n",
    "    # Create a new BAM file for writing the filtered reads\n",
    "    with pysam.AlignmentFile(filtered_bam_file, \"wb\", template=bam) as out_bam:\n",
    "        for read in bam:\n",
    "            # Skip unmapped reads\n",
    "            if read.is_unmapped:\n",
    "                continue\n",
    "            \n",
    "            # Get all reference positions covered by the read\n",
    "            reference_positions = read.get_reference_positions(full_length=True)\n",
    "            \n",
    "            # Count how many of the positions in the list are covered by this read\n",
    "            chrom = read.reference_name\n",
    "            if chrom in positions_by_chrom:\n",
    "                covered_positions = [pos for pos in reference_positions if pos in positions_by_chrom[chrom]]\n",
    "                \n",
    "                # Write the read to the output BAM if it covers more than one position\n",
    "                if len(covered_positions) > 1:\n",
    "                    out_bam.write(read)\n",
    "\n",
    "    # Close the BAM file\n",
    "    bam.close()\n",
    "\n",
    "## Extract the information from the filtered BAM file as a dataframe: columns should be read_name,\tcigar, chromosome, ref_positions, query_sequence, alts, refs, snp_pos\n",
    "def extract_info(filtered_bam_file):\n",
    "\n",
    "    # Open the filtered BAM file for reading\n",
    "    bam = pysam.AlignmentFile(filtered_bam_file, \"rb\")\n",
    "\n",
    "    # Create a list to store the read information\n",
    "    read_info = []\n",
    "\n",
    "    # Iterate over the reads in the BAM file\n",
    "    for read in bam:\n",
    "        # Get the read information\n",
    "        read_info.append({\n",
    "            \"read_name\": read.query_name,\n",
    "            \"cigar\": read.cigarstring,\n",
    "            \"chromosome\": read.reference_name,\n",
    "            \"ref_positions\": read.get_reference_positions(full_length=True),\n",
    "            \"query_sequence\": read.query_sequence,\n",
    "        })\n",
    "\n",
    "    # Close the BAM file\n",
    "    bam.close()\n",
    "\n",
    "    # Create a DataFrame from the read information\n",
    "    df = pd.DataFrame(read_info)\n",
    "    return df\n",
    "\n",
    "## Add in the base information from the lookup_df\n",
    "def add_base_info(df, positions_by_chrom):\n",
    "\n",
    "    def get_alleles(row):\n",
    "        chrom = row.chromosome\n",
    "        positions = row.ref_positions\n",
    "\n",
    "        alts = []\n",
    "        refs = []\n",
    "        snp_pos = []\n",
    "\n",
    "        for pos in positions:\n",
    "            if chrom in positions_by_chrom and pos in positions_by_chrom[chrom]:\n",
    "                alts.append(positions_by_chrom[chrom][pos]['alt'])\n",
    "                refs.append(positions_by_chrom[chrom][pos]['ref'])\n",
    "                snp_pos.append(pos)\n",
    "        return pd.Series([alts, refs, snp_pos])\n",
    "\n",
    "    df[\"ref_positions\"] = df[\"ref_positions\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    # Apply the function in parallel\n",
    "    df[['alts', 'refs','snp_pos']] = df.parallel_apply(get_alleles, axis=1)\n",
    "\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for processing the csv files\n",
    "## Add in the base info\n",
    "## For each position in snp_pos, find its index in ref_positions\n",
    "def find_bases(row):\n",
    "    snp_pos = row.snp_pos\n",
    "    ref_positions = row.ref_positions\n",
    "    bases = []\n",
    "    for pos in snp_pos:\n",
    "        if pos in ref_positions:\n",
    "            ind = ref_positions.index(pos) - 1\n",
    "            bases.append(row.query_sequence[ind])\n",
    "    return bases\n",
    "\n",
    "def filter_reads_col(row):\n",
    "        matches = 0\n",
    "        for i in range(len(row['bases'])):\n",
    "            if row['bases'][i] == row['alts'][i]:\n",
    "                matches += 1\n",
    "        return matches\n",
    "\n",
    "def filter_reads_ped(row):\n",
    "        matches = 0\n",
    "        for i in range(len(row['bases'])):\n",
    "            if row['bases'][i] == row['refs'][i]:\n",
    "                matches += 1\n",
    "        return matches\n",
    "\n",
    "def process_file(file_path, ecotype):\n",
    "     ## Load the dataframe\n",
    "    df = pd.read_csv(file_path, index_col=None)\n",
    "    ## Convert ref_positions to a list\n",
    "    df[\"ref_positions\"] = df[\"ref_positions\"].parallel_apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df[\"snp_pos\"] = df[\"snp_pos\"].parallel_apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df[\"alts\"] = df[\"alts\"].parallel_apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df[\"refs\"] = df[\"refs\"].parallel_apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    ## Add the base information\n",
    "    df['bases'] = df.parallel_apply(find_bases, axis=1)\n",
    "    ## Calculate the number of alt matches\n",
    "    if ecotype == \"col\":\n",
    "        df['alt_matches'] = df.parallel_apply(filter_reads_col, axis=1)\n",
    "    elif ecotype == \"ped\":\n",
    "        df['alt_matches'] = df.parallel_apply(filter_reads_ped, axis=1)\n",
    "    ## Save the dataframe again\n",
    "    df.to_csv(file_path, index = None)\n",
    "    ## Delete the dataframe from memory to save space\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the look up files\n",
    "lookup_df = pd.read_csv(\"/Users/tomkinsm/reanalysis-mobile-mrna/Data/thieme_snps_from_homograft.csv\", index_col=None)\n",
    "positions_by_chrom_set = get_positions_by_chrom_set(lookup_df)\n",
    "positions_by_chrom_dict = get_positions_by_chrom_dict(lookup_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_files = [\"C-C-root-FN.bam\",\"C-C-shoot-FN.bam\",\"P-P-root-FN.bam\",\"P-P-shoot-FN.bam\",\"Col-FN-root-1.bam\",\"Col-FN-root-2.bam\",\"Col-FN-root-3.bam\",\"Col-FN-shoot-1.bam\",\"Col-FN-shoot-2.bam\",\"Col-FN-shoot-3.bam\",\"Ped-FN-root-1.bam\",\"Ped-FN-root-2.bam\",\"Ped-FN-root-3.bam\",\"Ped-FN-shoot-1.bam\",\"Ped-FN-shoot-2.bam\",\"Ped-FN-shoot-3.bam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [27:45<00:00, 416.31s/it]\n"
     ]
    }
   ],
   "source": [
    "## >2 hours to run\n",
    "bam_directory = \"/Users/tomkinsm/mRNA_analysis/BAM/\"\n",
    "\n",
    "## Code to process the BAM files\n",
    "for file in tqdm(bam_files):\n",
    "    bam_file = bam_directory + file\n",
    "    filtered_bam_file = bam_file.replace(\".bam\", \"_filtered-2SNPs.bam\")\n",
    "    filter_reads_2SNPs(bam_file, positions_by_chrom_set, filtered_bam_file)\n",
    "    ## Create an index for the filtered BAM file\n",
    "    pysam.index(filtered_bam_file)\n",
    "    df = extract_info(filtered_bam_file)\n",
    "\n",
    "    ## Add the base information to the dataframe\n",
    "    df = add_base_info(df, positions_by_chrom_dict)\n",
    "\n",
    "    ## Save the dataframes to csv\n",
    "    df.to_csv(bam_file.replace(\".bam\", \".csv\"))\n",
    "\n",
    "    ## Remove the bam_file and dataframes from memory\n",
    "    del df\n",
    "    del bam_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [29:21<00:00, 110.06s/it] \n"
     ]
    }
   ],
   "source": [
    "## 29 minutes to run\n",
    "## Process the csv files\n",
    "for rep in tqdm(bam_files):\n",
    "    if rep.startswith(\"C\"):\n",
    "        process_file(bam_directory + rep.replace(\".bam\", \".csv\"), \"col\")\n",
    "    elif rep.startswith(\"P\"):\n",
    "        process_file(bam_directory + rep.replace(\".bam\", \".csv\"), \"ped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total reads: 1753179\n",
      "Number of reads where all SNPs match the alternate allele: 29\n",
      "Number of total reads: 1977539\n",
      "Number of reads where all SNPs match the alternate allele: 2\n",
      "Number of total reads: 610762\n",
      "Number of reads where all SNPs match the alternate allele: 235\n",
      "Number of total reads: 366704\n",
      "Number of reads where all SNPs match the alternate allele: 455\n",
      "Number of total reads: 832072\n",
      "Number of reads where all SNPs match the alternate allele: 81\n",
      "Number of total reads: 834696\n",
      "Number of reads where all SNPs match the alternate allele: 45\n",
      "Number of total reads: 891477\n",
      "Number of reads where all SNPs match the alternate allele: 43\n",
      "Number of total reads: 838014\n",
      "Number of reads where all SNPs match the alternate allele: 287\n",
      "Number of total reads: 1366447\n",
      "Number of reads where all SNPs match the alternate allele: 108\n"
     ]
    }
   ],
   "source": [
    "# Function to check if the first value in ref_positions is the same as the first value in snp_pos\n",
    "\n",
    "# Load the DataFrame\n",
    "directory_path = \"/Users/tomkinsm/mRNA_analysis/BAM/\"\n",
    "\n",
    "for file in tqdm(bam_files):\n",
    "    rep = file.replace(\".bam\", \".csv\")\n",
    "    file_path = directory_path + rep\n",
    "    test_df = pd.read_csv(file_path, index_col=None)\n",
    "\n",
    "    test_df['snp_pos'] = test_df['snp_pos'].parallel_apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    test_df['ref_positions'] = test_df['ref_positions'].parallel_apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    ## We filter the first and last 2 positions of each read\n",
    "    threshold_first = 2\n",
    "    threshold_final = 2\n",
    "\n",
    "    # Function to check if the first value in ref_positions is more than 3 bases away from the first value in snp_pos\n",
    "    # and the final value in snp_pos is not within 3 bases of the final position in ref_positions\n",
    "    def calculate_values(row):\n",
    "        if row['ref_positions'] and row['snp_pos']:\n",
    "            if row['ref_positions'][0] is not None and row['snp_pos'][0] is not None and row['ref_positions'][-1] is not None and row['snp_pos'][-1] is not None:\n",
    "                first_condition = abs(row['ref_positions'][0] - row['snp_pos'][0]) > threshold_first\n",
    "                final_condition = abs(row['ref_positions'][-1] - row['snp_pos'][-1]) > threshold_final\n",
    "                return first_condition and final_condition\n",
    "        return False\n",
    "\n",
    "    # Apply the function to each row and filter the DataFrame\n",
    "    filtered_df = test_df[test_df.apply(calculate_values, axis=1)].copy()\n",
    "    filtered_df['snp_pos'] = filtered_df['snp_pos'].parallel_apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    unique_reads = filtered_df['read_name'].nunique()\n",
    "    print(f\"Number of total reads: {unique_reads}\")\n",
    "\n",
    "    ## Obtain the number of reads where both bases match their respectives alts\n",
    "    matching_alts = filtered_df[filtered_df.apply(lambda row: row['alt_matches'] == len(row['snp_pos']), axis=1)]\n",
    "    unique_reads = matching_alts['read_name'].nunique()\n",
    "    print(f\"Number of reads where all SNPs match the alternate allele: {unique_reads}\")\n",
    "\n",
    "    ## Save the data\n",
    "    filtered_path = file_path.replace(\".csv\", \"_filtered.csv\")\n",
    "    matching_path = file_path.replace(\".csv\", \"_matching.csv\")\n",
    "    filtered_df.to_csv(filtered_path, index=None)\n",
    "    matching_alts.to_csv(matching_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in and analyse the data\n",
    "# Function to generate snp_names\n",
    "for file in bam_files:\n",
    "    file_path_matching = bam_directory + rep.replace(\".bam\", \"_matching.csv\")\n",
    "    file_path_filtered = bam_directory + rep.replace(\".bam\", \"_filtered.csv\")\n",
    "    matching = pd.read_csv(file_path_matching, index_col=None)\n",
    "    filtered = pd.read_csv(file_path_filtered, index_col=None)\n",
    "    matching_read = matching['read_name'].unique()\n",
    "    filtered_read = filtered['read_name'].unique()\n",
    "    # Apply the function to each row\n",
    "    print(rep)\n",
    "    print(f\"{len(matching_read)} out of {len(filtered_read)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCRootFN.csv\n",
      "1675\n",
      "CCShootFN.csv\n",
      "1797\n",
      "ColFNRoot1.csv\n",
      "674\n",
      "ColFNRoot2.csv\n",
      "677\n",
      "ColFNRoot3.csv\n",
      "755\n",
      "ColFNShoot1.csv\n",
      "721\n",
      "ColFNShoot2.csv\n",
      "1243\n",
      "ColFNShoot3.csv\n",
      "651\n",
      "PPRootFN.csv\n",
      "759\n",
      "PPShootFN.csv\n",
      "458\n",
      "PedFNRoot1.csv\n",
      "598\n",
      "PedFNRoot2.csv\n",
      "538\n",
      "PedFNRoot3.csv\n",
      "589\n",
      "PedFNShoot1.csv\n",
      "564\n",
      "PedFNShoot2.csv\n",
      "735\n",
      "PedFNShoot3.csv\n",
      "631\n"
     ]
    }
   ],
   "source": [
    "## Get the number of reads with at least 1 SNP matching the alternate allele, but not all SNPs\n",
    "for file in bam_files:\n",
    "    file_path = bam_directory + file.replace(\".bam\", \"_filtered_conflicting.csv\")\n",
    "    conflicting = pd.read_csv(file_path, index_col=None)\n",
    "    print(len(conflicting['read_name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
